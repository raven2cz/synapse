# AI-Powered Parameter Extraction ‚Äì Specification & Implementation Plan

## Context

Tato specifikace shrnuje v√Ωsledky testov√°n√≠ a n√°vrh implementace AI-powered extrakce
generaƒçn√≠ch parametr≈Ø z model descriptions na platform√°ch jako CivitAI a HuggingFace.
Parametry mohou b√Ωt relevantn√≠ pro libovoln√© Stable Diffusion UI (AUTOMATIC1111,
Forge/Neo, ComfyUI, InvokeAI, Fooocus aj.) a libovolnou modelovou architekturu
(SD 1.5, SDXL, SD3, Flux, Cascade, PixArt, Hunyuan, LTX-Video aj.).
Dokument slou≈æ√≠ jako reference pro implementaci v r√°mci existuj√≠c√≠ Python aplikace,
kter√° aktu√°lnƒõ pou≈æ√≠v√° rule-based (regexp) parser.

**Autor:** raven2cz + Claude Opus 4.5  
**Datum testov√°n√≠:** √önor 2026  
**Prost≈ôed√≠:** Arch Linux, Fish shell, RTX 5070 Ti (16 GB VRAM)  
**Prompt verze:** V2 (`58ee6eb2db8f`) ‚Äî zahrnuje 5 vylep≈°en√≠ z benchmark V1

---

## 1. Testovan√© AI providery

Vol√°n√≠ AI prob√≠h√° p≈ôes CLI subprocessy ‚Äì ≈æ√°dn√© API kl√≠ƒçe, ≈æ√°dn√© platby za tokeny.
U≈æivatel vyu≈æ√≠v√° sv√© st√°vaj√≠c√≠ subskripce (Claude Max, Gemini Pro) a lok√°ln√≠ Ollama.

### 1.1 Claude Code (Anthropic)

- **P≈ô√≠kaz:** `claude --print --model <model> "<prompt>"`
- **Subskripce:** Claude Max (t√Ωdenn√≠ limit zpr√°v)
- **Doporuƒçen√Ω model:** `claude-sonnet-4-20250514`
  - Sonnet je pro extrakci optim√°ln√≠ ‚Äì ≈°et≈ô√≠ kv√≥tu oproti Opus
  - Dal≈°√≠ varianty: `claude-haiku-4-5-20251001` (nejlevnƒõj≈°√≠), `claude-opus-4-5-20251101` (zbyteƒçn√Ω overkill)

### 1.2 Gemini CLI (Google)

- **P≈ô√≠kaz:** `gemini --model <model> -p "<prompt>"`
- **Subskripce:** Google AI Pro/Ultra (prakticky neomezen√© pou≈æit√≠)
- **Doporuƒçen√Ω model:** `gemini-3-pro-preview`
  - Vy≈æaduje zapnut√≠ "Preview features" v `/settings` Gemini CLI
  - Dal≈°√≠ varianty: `gemini-3-flash-preview` (rychlej≈°√≠), `gemini-2.5-pro` (stabiln√≠ GA fallback)

### 1.3 Ollama (lok√°ln√≠)

- **P≈ô√≠kaz:** `ollama run <model> "<prompt>"`
- **Instalace (Arch):** `yay -S ollama-cuda`
- **Spu≈°tƒõn√≠:** `ollama serve` (manu√°lnƒõ, ne jako systemd service ‚Äì ≈°et≈ô√≠ VRAM)
- **Doporuƒçen√Ω model:** `qwen2.5:14b` (~9 GB VRAM)
  - 7b verze selhala na slo≈æitƒõj≈°√≠ch descriptions (broken JSON)
  - 14b na RTX 5070 Ti bƒõ≈æ√≠ komfortnƒõ a generuje validn√≠ v√Ωstup
  - 32b se nevejde do 16 GB VRAM (pot≈ôebuje ~18-20 GB)

---

## 2. V√Ωsledky testov√°n√≠

### 2.1 Pilotn√≠ testy (2 descriptions, prompt V1)

Dva poƒç√°teƒçn√≠ testy ovƒõ≈ôily z√°kladn√≠ funkƒçnost. Strukturovan√Ω vstup (jednoduch√© parametry):
100% √∫spƒõ≈°nost u v≈°ech provider≈Ø. Re√°ln√° CivitAI description (GhostMix V2.0):
Claude a Gemini 9/9 parametr≈Ø, Ollama 9/9 ale nedetekoval base_model.

### 2.2 Benchmark V1 ‚Äî 13 model≈Ø, prompt V1 (`57dcf524b0a0`)

Komplexn√≠ benchmark na 13 diverzn√≠ch CivitAI descriptions:

**Testovac√≠ sada:** 5√ó SD 1.5, 2√ó SDXL, 4√ó Flux, 2√ó Pony, 1√ó Universal upscaler.
Typy: 9√ó Checkpoint, 3√ó LoRA, 1√ó Upscaler. Jazyky: EN, EN/CN, EN/JP.
Rozsah popis≈Ø: 262‚Äì1424 znak≈Ø.

| Provider | √öspƒõ≈°nost | √ò ƒåas | √ò Kl√≠ƒç≈Ø | JSON chyby | Timeouty |
|----------|-----------|-------|---------|------------|----------|
| Ollama qwen2.5:14b | 13/13 | **2.9s** | 10.1 | 0 | 0 |
| Gemini 3 Pro | 13/13 | 21.6s | 8.5 | 0 | 0 |
| Claude Sonnet 4 | 13/13 | 8.1s | **11.5** | 0 | 0 |

**Identifikovan√© probl√©my:**

| Probl√©m | Provider | Z√°va≈ænost | ≈òe≈°en√≠ v V2 |
|---------|----------|-----------|-------------|
| Halucinace base_model (MeinaMix ‚Üí "SDXL" m√≠sto SD 1.5) | Ollama | üî¥ Kritick√© | Rule 9: extract only if explicit |
| Placeholder "..." zkop√≠rov√°n z p≈ô√≠kladu | Ollama | üü° St≈ôedn√≠ | Rule 10: never copy placeholders |
| Mezery v n√°zvech kl√≠ƒç≈Ø (`"Hires upscaler"`) | Gemini | üü° St≈ôedn√≠ | Rule 2: enforce snake_case |
| Over-nesting (MeinaMix: 6 kl√≠ƒç≈Ø m√≠sto 14) | Gemini | üü° St≈ôedn√≠ | Rule 3: anti-grouping |
| String ranges ("20-30" m√≠sto {min, max}) | Ollama | üü¢ N√≠zk√© | Rule 4: numeric ranges |
| Slab√° extrakce minim√°ln√≠ch popis≈Ø (SynthwavePunk: 2 kl√≠ƒçe) | Ollama | üü° St≈ôedn√≠ | Rule 1: extract everything |

### 2.3 Prompt V2 ‚Äî 5 vylep≈°en√≠

Na z√°kladƒõ benchmark V1 byl prompt roz≈°√≠≈ôen z 6 na 10 pravidel:

1. **Minim√°ln√≠ popisy** (Rule 1): Explicitnƒõ instruuje extrahovat v≈°e i z kr√°tk√Ωch popis≈Ø.
2. **snake_case** (Rule 2): Vynucuje konzistentn√≠ pojmenov√°n√≠ kl√≠ƒç≈Ø bez mezer.
3. **Anti-grouping** (Rule 3): Zakazuje zabalen√≠ nesouvisej√≠c√≠ch parametr≈Ø do wrapper kl√≠ƒç≈Ø.
4. **Numerick√© rozsahy** (Rule 4): Vy≈æaduje `{min, max}` s ƒç√≠sly, ne stringy.
5. **Typov√°n√≠ ƒç√≠sel** (Rule 8): ƒå√≠sla jako JSON numbers, ne stringy.
6. **Base model guard** (Rule 9): Extrahovat jen explicitnƒõ uveden√©, neinferovat.
7. **Anti-placeholder** (Rule 10): Nekop√≠rovat "..." z p≈ô√≠kladu.

P≈ô√≠klad v promptu zmƒõnƒõn z `{"base_model": "...", "some_parameter": 7, "tips": [...]}` na
`{"base_model": "SDXL 1.0", "sampler": ["DPM++ 2M"], "steps": {"min": 20, "max": 30}}` ‚Äî
realistick√© hodnoty m√≠sto placeholder≈Ø.

### 2.4 Roz≈°√≠≈ôen√© evaluaƒçn√≠ metriky (V2)

Benchmark V2 mƒõ≈ô√≠ kromƒõ p≈Øvodn√≠ch metrik tak√©:

| Metrika | Co mƒõ≈ô√≠ |
|---------|---------|
| `naming_score` | % kl√≠ƒç≈Ø v platn√©m snake_case (0‚Äì100) |
| `numeric_typing_score` | % ƒç√≠sel jako JSON number, ne string (0‚Äì100) |
| `placeholder_count` | Poƒçet "..." hodnot zkop√≠rovan√Ωch z promptu |
| `avg_nesting_depth` | Pr≈Ømƒõrn√° hloubka vno≈ôov√°n√≠ (ni≈æ≈°√≠ = flat = lep≈°√≠ parsovatelnost) |
| `wrapper_keys` | Kl√≠ƒçe, kter√© zabaluj√≠ ‚â•4 nesouvisej√≠c√≠ch parametr≈Ø |

### 2.5 Kvalitativn√≠ srovn√°n√≠ (aktualizov√°no po V1 benchmarku)

| Aspekt | Claude Sonnet 4 | Gemini 3 Pro | Ollama 14b |
|--------|-----------------|--------------|------------|
| Hloubka extrakce | ü•á √ò 11.5 kl√≠ƒç≈Ø | ü•â √ò 8.5 kl√≠ƒç≈Ø | ü•à √ò 10.1 kl√≠ƒç≈Ø |
| Rychlost | ü•à √ò 8.1s | ü•â √ò 21.6s | ü•á √ò 2.9s |
| Konzistence kl√≠ƒç≈Ø | ü•á 100% snake_case | ü•â 12 kl√≠ƒç≈Ø s mezerami | ü•à 3 camelCase |
| Typov√°n√≠ hodnot | ü•á ƒå√≠seln√© rozsahy jako {min,max} | ü•â ƒåasto string ranges | ü•à Obƒças string ranges |
| Struktur√°ln√≠ p≈ô√≠stup | ü•á Balanced flat+nested | ü•â P≈ô√≠li≈° hlubok√Ω nesting | ü•à Flat/denormalized |
| Minim√°ln√≠ popisy | ü•á 7 kl√≠ƒç≈Ø (SynthwavePunk) | ü•à 6 kl√≠ƒç≈Ø | ü•â 2 kl√≠ƒçe |
| Halucinace | ü•á ≈Ω√°dn√© | ü•á ≈Ω√°dn√© | ü•â base_model halucinace |
| Celkov√° kvalita | ü•á | ü•à | ü•â (pro minim√°ln√≠ popisy) |

---

## 3. Doporuƒçen√° priorita provider≈Ø

S ohledem na v√Ωsledky benchmarku V1 a praktick√° omezen√≠ subskripc√≠:

### Priorita 1: Ollama qwen2.5:14b (lok√°ln√≠) ‚Äî pro bulk operace
- **Nejrychlej≈°√≠** na GPU (√ò 2.9s, medi√°n 3.2s)
- **≈Ω√°dn√© limity**, ≈æ√°dn√° kv√≥ta, offline
- U≈æivatel√© ComfyUI typicky maj√≠ GPU
- **Rizika:** Halucinace base_model, slab√Ω na minim√°ln√≠ch popisech ‚Äî proto V2 prompt
- Doporuƒçen√≠: Post-processing validace na kritick√Ωch pol√≠ch (base_model ovƒõ≈ôit z metadat)

### Priorita 2: Gemini 3 Pro (cloud fallback)
- **Neomezen√© pou≈æit√≠** s Pro/Ultra subskripc√≠
- Kvalitn√≠ v√Ωstup, ale nejpomalej≈°√≠ (√ò 21.6s) a tendence k over-nesting
- V2 prompt by mƒõl zlep≈°it flat strukturu a snake_case naming
- Gemini i Ollama mohou vracet markdown fences ‚Äì **parser mus√≠ v≈ædy stripovat fences**

### Priorita 3: Claude Sonnet 4 (pr√©miov√Ω fallback)
- **Nejvy≈°≈°√≠ kvalita** extrakce (√ò 11.5 kl√≠ƒç≈Ø, 100% snake_case, ≈æ√°dn√© halucinace)
- **Omezen√° kv√≥ta** (Max = t√Ωdenn√≠ limit) ‚Äì ≈°et≈ôit na d≈Øle≈æitƒõj≈°√≠ pr√°ci
- Optim√°ln√≠ pro v√Ωjimeƒçnƒõ slo≈æit√© descriptions nebo validaci v√Ωsledk≈Ø jin√Ωch provider≈Ø

### Rule-based (regexp) ‚Äì v≈ædy p≈ô√≠tomn√Ω
- St√°vaj√≠c√≠ implementace, ≈æ√°dn√© z√°vislosti
- Funguje pro jednodu≈°e strukturovan√© descriptions
- Automatick√Ω fallback, kdy≈æ ≈æ√°dn√Ω AI provider nen√≠ dostupn√Ω/nakonfigurovan√Ω

---

## 4. Architektura integrace

### 4.1 Settings (u≈æivatelsk√° konfigurace)

```
[AI Services]

Enabled AI Providers:
  [‚úì] Ollama (local)        Model: [qwen2.5:14b    ]  Endpoint: [localhost:11434]
  [‚úì] Gemini CLI             Model: [gemini-3-pro-preview]
  [ ] Claude Code            Model: [claude-sonnet-4-20250514]

Fallback chain order: Ollama ‚Üí Gemini ‚Üí Rule-based
  (≈ôad√≠ se automaticky podle v√Ω≈°e uveden√© priority,
   nebo u≈æivatel m≈Ø≈æe p≈ôe≈ôadit manu√°lnƒõ)

[Advanced]
  CLI timeout (sec):                [60     ]
  Always fallback to rule-based:    [‚úì]
  Cache AI results:                 [‚úì]  (stejn√Ω description ‚Üí stejn√Ω v√Ωsledek)
```

### 4.2 Detekce dostupnosti provider≈Ø

P≈ôi startu aplikace (nebo p≈ôi otev≈ôen√≠ Settings) automaticky ovƒõ≈ôit:

```python
import shutil

def detect_available_providers() -> dict:
    """Detect which AI CLI tools are installed and accessible."""
    providers = {}

    # Claude Code
    if shutil.which("claude"):
        providers["claude"] = {"available": True, "command": "claude"}

    # Gemini CLI
    if shutil.which("gemini"):
        providers["gemini"] = {"available": True, "command": "gemini"}

    # Ollama
    if shutil.which("ollama"):
        # Additionally check if server is running
        providers["ollama"] = {"available": True, "command": "ollama"}

    return providers
```

V Settings zobrazit jen dostupn√© providery a u nedostupn√Ωch ≈°ed√Ω text s instrukc√≠
pro instalaci.

### 4.3 Vol√°n√≠ AI provideru

Ka≈æd√Ω provider se vol√° jako subprocess:

```python
import subprocess

def call_ai_provider(provider: str, model: str, prompt: str, timeout: int = 60) -> str:
    """
    Call an AI CLI tool as a subprocess and return the response text.

    Args:
        provider: One of 'claude', 'gemini', 'ollama'.
        model: Model identifier string.
        prompt: The extraction prompt.
        timeout: Maximum wait time in seconds.

    Returns:
        Raw response text from the AI provider.

    Raises:
        subprocess.TimeoutExpired: If the provider takes too long.
        RuntimeError: If the provider returns a non-zero exit code.
    """
    commands = {
        "claude": ["claude", "--print", "--model", model, prompt],
        "gemini": ["gemini", "--model", model, "-p", prompt],
        "ollama": ["ollama", "run", model, prompt],
    }

    result = subprocess.run(
        commands[provider],
        capture_output=True,
        text=True,
        timeout=timeout,
    )

    if result.returncode != 0:
        raise RuntimeError(f"{provider} failed: {result.stderr.strip()}")

    return result.stdout.strip()
```

### 4.4 Parsov√°n√≠ odpovƒõdi

Gemini i Ollama mohou vracet JSON obalen√Ω v markdown fences ‚Äì **v≈ædy stripovat**:

```python
import json

def parse_ai_response(response: str) -> dict:
    """
    Parse JSON from AI response, stripping markdown fences if present.

    Args:
        response: Raw text response from AI provider.

    Returns:
        Parsed dictionary with extracted parameters.

    Raises:
        json.JSONDecodeError: If the response is not valid JSON.
    """
    text = response.strip()

    # Strip markdown code fences (```json ... ``` or ``` ... ```)
    if text.startswith("```"):
        text = text.split("\n", 1)[1] if "\n" in text else text[3:]
    if text.endswith("```"):
        text = text.rsplit("```", 1)[0]

    return json.loads(text.strip())
```

### 4.5 Fallback chain

```python
def extract_parameters(description: str, settings: Settings) -> dict:
    """
    Extract generation parameters using the configured fallback chain.

    Tries each enabled provider in priority order. Falls back to rule-based
    extraction if all AI providers fail or are disabled.

    Args:
        description: Raw HTML description from CivitAI/ComfyUI.
        settings: User's application settings with provider configuration.

    Returns:
        Dictionary with extracted generation parameters.
    """
    prompt = build_extraction_prompt(description)

    for provider_cfg in settings.ai_fallback_chain:
        if not provider_cfg.enabled:
            continue

        try:
            response = call_ai_provider(
                provider=provider_cfg.provider,
                model=provider_cfg.model,
                prompt=prompt,
                timeout=settings.ai_timeout,
            )
            result = parse_ai_response(response)
            result["_extracted_by"] = provider_cfg.provider
            return result

        except (subprocess.TimeoutExpired, RuntimeError, json.JSONDecodeError) as e:
            logger.warning(f"AI provider {provider_cfg.provider} failed: {e}")
            continue

    # Final fallback: rule-based extraction
    logger.info("All AI providers failed, using rule-based extraction")
    result = extract_parameters_regexp(description)
    result["_extracted_by"] = "rule-based"
    return result
```

### 4.6 Prompt template

#### Designov√© principy

Prompt mus√≠ b√Ωt **obecn√Ω a otev≈ôen√Ω**, proto≈æe:

1. **Ekosyst√©m UI je fragmentovan√Ω:** AUTOMATIC1111, Forge/Neo, ComfyUI, InvokeAI,
   Fooocus, SD.Next, DrawThings ‚Äì ka≈æd√Ω pou≈æ√≠v√° jin√© n√°zvy parametr≈Ø a jin√© defaults.
2. **Architektury model≈Ø se rychle mƒõn√≠:** SD 1.5, SDXL, SD3, Flux, Cascade, PixArt,
   Hunyuan, LTXV (video)... ka≈æd√° generace p≈ôin√°≈°√≠ nov√© parametry.
3. **Auto≈ôi descriptions jsou nep≈ôedv√≠dateln√≠:** nƒõkdo nap√≠≈°e strukturovan√Ω seznam,
   jin√Ω zabal√≠ parametry do vypr√°vƒõn√≠, t≈ôet√≠ mix EN/CN/JP.
4. **ƒåasov√Ω horizont:** za p≈Øl roku mohou existovat parametry, o kter√Ωch dnes nev√≠me.

Proto prompt **NESM√ç enumerovat konkr√©tn√≠ kl√≠ƒçe** ‚Äì jen uk√°zat form√°t hodnot
jako p≈ô√≠klad a explicitnƒõ ≈ô√≠ct "extrahuj V≈†E, co tam je".

```python
EXTRACTION_PROMPT = """\
You are an expert in AI image and video generation ecosystems, including all \
Stable Diffusion UIs (AUTOMATIC1111, Forge, ComfyUI, InvokeAI, Fooocus, etc.) \
and all model architectures (SD 1.5, SDXL, SD3, Flux, Cascade, PixArt, \
Hunyuan, LTX-Video, and any future architectures).

Your task: analyze the model description below and extract EVERY generation \
parameter, setting, recommendation, compatibility note, or workflow tip \
mentioned by the author. The description may be in any language or a mix of \
languages, and may contain HTML markup ‚Äî ignore the markup, focus on content.

Rules:
1. Extract ALL parameters you find, not just common ones. Even for very \
short descriptions, extract everything available: merge components, ratios, \
trigger words, compatibility notes, and any implied usage recommendations.
2. Use consistent snake_case for ALL key names (e.g. "cfg_scale", \
"clip_skip", "hires_fix"). Never use spaces or PascalCase in keys. \
You may still follow the author's terminology for the concept name itself \
(e.g. "guidance" vs "cfg_scale"), but always format the key in snake_case.
3. Each distinct parameter MUST be its own top-level key. Do NOT group \
all parameters under a single wrapper key like "recommended_parameters" \
or "settings". If an author lists sampler, steps, cfg as separate items, \
they should be separate top-level keys in your output. Sub-objects are \
fine for closely related sub-values (e.g. hires_fix: {upscaler, steps, \
denoising}) but NOT for wrapping unrelated parameters together.
4. For parameters with numeric ranges, ALWAYS use {"min": <number>, \
"max": <number>} with actual numeric values ‚Äî not strings. If only a \
recommended value is given with a range hint, use \
{"recommended": <number>, "min": <number>, "max": <number>}.
5. For parameters with alternatives, use arrays: ["DPM++ 2M", "Euler a"].
6. Include compatibility info, warnings, and tips as separate keys.
7. If the author mentions specific models, LoRAs, embeddings, VAEs, \
upscalers, ControlNets, or any other auxiliary resources, include them.
8. Preserve numeric values exactly as stated (don't round or convert). \
Always output numbers as JSON numbers, not strings (7 not "7").
9. For "base_model": extract ONLY if the author explicitly states it. \
Do NOT guess or infer the base architecture from context.
10. NEVER copy placeholder values like "..." from this prompt. Extract \
real values from the description or omit the key entirely.

Return ONLY valid JSON. No markdown fences, no explanation, no preamble.

The following is a MINIMAL ILLUSTRATIVE example of the output FORMAT ‚Äî your \
actual output should contain whatever keys match the description content, \
which may be completely different from this example:

{"base_model": "SDXL 1.0", "sampler": ["DPM++ 2M"], "steps": {"min": 20, "max": 30}}

Omit keys where no information is found. Add any keys the description warrants.

Description:
"""


def build_extraction_prompt(description: str) -> str:
    """Build the full extraction prompt with the description appended."""
    return f"{EXTRACTION_PROMPT}{description}"
```

#### Proƒç je prompt V2 navr≈æen takto

| Designov√© rozhodnut√≠ | D≈Øvod | Nov√© ve V2 |
|---------------------|-------|------------|
| V√Ωƒçet UI a architektur ve scope | AI v√≠, ≈æe m√° hledat parametry pro cel√Ω ekosyst√©m | ‚Äì |
| "Extract EVERY parameter" + minim√°ln√≠ popisy | Zabr√°nit tendenci AI zamƒõ≈ôit se jen na bƒõ≈æn√© parametry | ‚úÖ Rule 1 |
| snake_case enforcement | Konzistentn√≠ kl√≠ƒçe pro downstream parsing, bez mezer | ‚úÖ Rule 2 |
| Anti-grouping instrukce | Ka≈æd√Ω parametr jako top-level kl√≠ƒç, ne zabalen√Ω v wrapperu | ‚úÖ Rule 3 |
| Numerick√© rozsahy jako {min, max} | Strojovƒõ parsovateln√© rozsahy m√≠sto string≈Ø | ‚úÖ Rule 4 |
| ƒå√≠sla jako JSON numbers | `7` ne `"7"` ‚Äì spr√°vn√© typov√°n√≠ | ‚úÖ Rule 8 |
| Base model guard | Extrahovat jen explicitn√≠, ne inferovat (‚Üí halucinace) | ‚úÖ Rule 9 |
| Anti-placeholder | Nezkopirov√°vat "..." z p≈ô√≠kladu promptu | ‚úÖ Rule 10 |
| Realistick√Ω p≈ô√≠klad m√≠sto "..." | `{"base_model": "SDXL 1.0", "sampler": [...], "steps": {...}}` | ‚úÖ |
| Minim√°ln√≠ JSON p≈ô√≠klad (3 kl√≠ƒçe) | Ukazuje form√°t, ale je z√°mƒõrnƒõ trivi√°ln√≠ | ‚Äì |
| "which may be completely different" | Explicitn√≠ sign√°l, ≈æe p≈ô√≠klad nen√≠ template | ‚Äì |
| ≈Ω√°dn√Ω fixn√≠ seznam kl√≠ƒç≈Ø | Future-proof ‚Äì nov√© parametry se extrahuj√≠ automaticky | ‚Äì |

---

## 5. Cache v√Ωsledk≈Ø

Stejn√° description ‚Üí stejn√© parametry. Cache ≈°et≈ô√≠ kv√≥tu i ƒças:

```python
import hashlib

def get_cache_key(description: str) -> str:
    """Generate a deterministic cache key for a description."""
    return hashlib.sha256(description.encode()).hexdigest()[:16]
```

Cache ulo≈æit jako JSON soubor vedle pack.json nebo v centr√°ln√≠ cache directory.
Invalidace: p≈ôi zmƒõnƒõ description hashe nebo manu√°lnƒõ z UI.

---

## 6. Budouc√≠ roz≈°√≠≈ôen√≠

Tato architektura (Settings ‚Üí povolen√© AI slu≈æby ‚Üí fallback chain ‚Üí toolchain v√Ωbƒõr)
je navr≈æena obecnƒõ a lze ji pou≈æ√≠t i pro dal≈°√≠ AI-powered funkce v aplikaci:

- **Generov√°n√≠ workflow** z extrahovan√Ωch parametr≈Ø (ComfyUI JSON, A1111 settings)
- **P≈ôeklad descriptions** (libovoln√Ω jazyk ‚Üí EN)
- **Automatick√Ω tagging** model≈Ø podle obsahu description
- **Doporuƒçen√≠ kompatibiln√≠ch LoRA/embeddings** k checkpointu
- **Anal√Ωza preview obr√°zk≈Ø** (popis stylu, kvalita, detekce artefakt≈Ø)

Pro ka≈æd√Ω use case se toolchain vybere automaticky podle priority nastaven√©
u≈æivatelem v Settings, p≈ôiƒçem≈æ po≈ôad√≠ se m≈Ø≈æe li≈°it podle n√°roƒçnosti √∫lohy.
Jednodu≈°≈°√≠ √∫lohy (tagging, p≈ôeklad) mohou preferovat Ollama, slo≈æitƒõj≈°√≠
(workflow generov√°n√≠, anal√Ωza obr√°zk≈Ø) mohou preferovat cloud providery.

---

## 7. Shrnut√≠

| Rozhodnut√≠ | Volba | D≈Øvod |
|------------|-------|-------|
| Nejlep≈°√≠ kvalita | Claude Sonnet 4 | √ò 11.5 kl√≠ƒç≈Ø, 100% snake_case, ≈æ√°dn√© halucinace |
| Prim√°rn√≠ provider | Ollama qwen2.5:14b | √ò 2.9s, offline, bez limit≈Ø (s post-validac√≠) |
| Cloud fallback | Gemini 3 Pro | Neomezen√© s Pro subskripc√≠ |
| Zp≈Øsob vol√°n√≠ | subprocess CLI | ≈Ω√°dn√© API kl√≠ƒçe, vyu≈æ√≠v√° existuj√≠c√≠ subskripce |
| Prompt verze | V2 (`58ee6eb2db8f`) | 10 pravidel, ≈ôe≈°√≠ halucinace, naming, nesting |
| JSON parsing | V≈ædy stripovat fences | Gemini a Ollama je ƒçasto p≈ôid√°vaj√≠ |
| Cache | SHA-256 hash description | ≈†et≈ô√≠ kv√≥tu, zrychluje opakovan√© vol√°n√≠ |
| Settings UX | Auto-detect + manu√°ln√≠ override | Zobrazit jen dostupn√© providery |
| Evaluace | 5 nov√Ωch metrik | naming_score, numeric_typing, placeholders, nesting, wrappers |
